{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde7db8b-4816-47bd-be5a-a352c728d2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n",
      "Data berlabel: 4000 sampel\n",
      "Data tanpa label: 1000 sampel\n",
      "\n",
      "=== Training Baseline Model ===\n",
      "Baseline Model Results:\n",
      "RMSE: 3.9331\n",
      "Accuracy: 0.6212\n",
      "F1-Score: 0.5938\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.72       106\n",
      "           1       0.33      0.14      0.20        29\n",
      "           2       0.58      0.83      0.68       195\n",
      "           3       0.48      0.18      0.26        56\n",
      "           4       0.65      0.22      0.33        49\n",
      "           5       0.72      0.88      0.79       171\n",
      "           6       0.51      0.47      0.49       124\n",
      "           7       0.56      0.41      0.48        70\n",
      "\n",
      "    accuracy                           0.62       800\n",
      "   macro avg       0.57      0.48      0.49       800\n",
      "weighted avg       0.61      0.62      0.59       800\n",
      "\n",
      "\n",
      "=== Training PyTorch Model ===\n",
      "Epoch [1/10], Loss: 1.9293\n",
      "Epoch [2/10], Loss: 1.5231\n",
      "Epoch [3/10], Loss: 1.1248\n",
      "Epoch [4/10], Loss: 0.8423\n",
      "Epoch [5/10], Loss: 0.6310\n",
      "Epoch [6/10], Loss: 0.4738\n",
      "Epoch [7/10], Loss: 0.3433\n",
      "Epoch [8/10], Loss: 0.2521\n",
      "Epoch [9/10], Loss: 0.1860\n",
      "Epoch [10/10], Loss: 0.1390\n",
      "\n",
      "PyTorch Model Results:\n",
      "RMSE: 3.8159\n",
      "Accuracy: 0.6088\n",
      "F1-Score: 0.5964\n",
      "\n",
      "=== Self-Training dengan Data Unlabeled ===\n",
      "Self-Training Epoch [1/10], Loss: 1.8489\n",
      "Self-Training Epoch [2/10], Loss: 1.3209\n",
      "Self-Training Epoch [3/10], Loss: 0.9217\n",
      "Self-Training Epoch [4/10], Loss: 0.6666\n",
      "Self-Training Epoch [5/10], Loss: 0.4771\n",
      "Self-Training Epoch [6/10], Loss: 0.3390\n",
      "Self-Training Epoch [7/10], Loss: 0.2382\n",
      "Self-Training Epoch [8/10], Loss: 0.1706\n",
      "Self-Training Epoch [9/10], Loss: 0.1209\n",
      "Self-Training Epoch [10/10], Loss: 0.0907\n",
      "\n",
      "Self-Trained Model Results:\n",
      "RMSE: 3.8018\n",
      "Accuracy: 0.6175\n",
      "F1-Score: 0.6078\n",
      "\n",
      "=== RINGKASAN HASIL ===\n",
      "Model                RMSE       Accuracy   F1-Score  \n",
      "--------------------------------------------------\n",
      "Baseline (LR)        3.9331     0.6212     0.5938    \n",
      "PyTorch              3.8159     0.6088     0.5964    \n",
      "Self-Trained         3.8018     0.6175     0.6078    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Cek ketersediaan PyTorch dan GPU\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# 2. Load dataset (ganti dengan path dataset Anda)\n",
    "# Diasumsikan dataset memiliki kolom 'text' dan 'label'\n",
    "df = pd.read_csv('train.csv')  # Ganti dengan nama file Anda\n",
    "\n",
    "# Pastikan label di-encode menjadi numerik\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# 3. Split data: 80% berlabel, 20% tanpa label (simulasi semi-supervised)\n",
    "df_labeled, df_unlabeled = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "df_unlabeled = df_unlabeled.copy()\n",
    "df_unlabeled['label'] = np.nan  # Hapus label untuk data unlabeled\n",
    "\n",
    "print(f\"Data berlabel: {len(df_labeled)} sampel\")\n",
    "print(f\"Data tanpa label: {len(df_unlabeled)} sampel\")\n",
    "\n",
    "# 4. Preprocessing dan vektorisasi teks\n",
    "# Hapus stop_words atau gunakan stop words Indonesia jika tersedia\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # stop_words dihapus\n",
    "X_labeled = vectorizer.fit_transform(df_labeled['text'])\n",
    "y_labeled = df_labeled['label']\n",
    "\n",
    "# Split data berlabel menjadi train dan test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_labeled, y_labeled, test_size=0.2, stratify=y_labeled, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Latih model baseline (Logistic Regression)\n",
    "print(\"\\n=== Training Baseline Model ===\")\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi dan evaluasi\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test)\n",
    "\n",
    "# Hitung metrik - PERBAIKAN DI SINI\n",
    "# Gunakan probabilitas kelas yang benar untuk RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_proba[range(len(y_test)), y_test]))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Baseline Model Results:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 6. Persiapan data untuk model PyTorch\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Konversi data ke format PyTorch\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Buat DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 7. Training model PyTorch\n",
    "print(\"\\n=== Training PyTorch Model ===\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(y_train.unique())\n",
    "model = TextClassifier(input_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# 8. Evaluasi model PyTorch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test_tensor.to(device)\n",
    "    outputs = model(X_test_device)\n",
    "    _, y_pred_torch = torch.max(outputs, 1)\n",
    "    y_pred_proba_torch = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Hitung metrik - PERBAIKAN DI SINI\n",
    "rmse_torch = np.sqrt(mean_squared_error(y_test, y_pred_proba_torch[range(len(y_test)), y_test]))\n",
    "accuracy_torch = accuracy_score(y_test, y_pred_torch.cpu().numpy())\n",
    "f1_torch = f1_score(y_test, y_pred_torch.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"\\nPyTorch Model Results:\")\n",
    "print(f\"RMSE: {rmse_torch:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_torch:.4f}\")\n",
    "print(f\"F1-Score: {f1_torch:.4f}\")\n",
    "\n",
    "# 9. Self-training dengan data unlabeled (semi-supervised learning)\n",
    "print(\"\\n=== Self-Training dengan Data Unlabeled ===\")\n",
    "# Vektorisasi data unlabeled\n",
    "X_unlabeled = vectorizer.transform(df_unlabeled['text'])\n",
    "X_unlabeled_tensor = torch.tensor(X_unlabeled.toarray(), dtype=torch.float32).to(device)\n",
    "\n",
    "# Prediksi pseudo-label\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_unlabeled = model(X_unlabeled_tensor)\n",
    "    _, pseudo_labels = torch.max(outputs_unlabeled, 1)\n",
    "\n",
    "# Gabungkan data berlabel dan pseudo-label\n",
    "X_combined = torch.cat([X_train_tensor, X_unlabeled_tensor.cpu()], dim=0)\n",
    "y_combined = torch.cat([y_train_tensor, pseudo_labels.cpu()], dim=0)\n",
    "\n",
    "# Latih ulang model dengan data gabungan\n",
    "combined_dataset = TensorDataset(X_combined, y_combined)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model2 = TextClassifier(input_size, num_classes).to(device)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in combined_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        outputs = model2(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Self-Training Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(combined_loader):.4f}')\n",
    "\n",
    "# Evaluasi model setelah self-training\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test_tensor.to(device)\n",
    "    outputs = model2(X_test_device)\n",
    "    _, y_pred_combined = torch.max(outputs, 1)\n",
    "    y_pred_proba_combined = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Hitung metrik - PERBAIKAN DI SINI\n",
    "rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_proba_combined[range(len(y_test)), y_test]))\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined.cpu().numpy())\n",
    "f1_combined = f1_score(y_test, y_pred_combined.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"\\nSelf-Trained Model Results:\")\n",
    "print(f\"RMSE: {rmse_combined:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_combined:.4f}\")\n",
    "print(f\"F1-Score: {f1_combined:.4f}\")\n",
    "\n",
    "# 10. Ringkasan hasil\n",
    "print(\"\\n=== RINGKASAN HASIL ===\")\n",
    "print(f\"{'Model':<20} {'RMSE':<10} {'Accuracy':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Baseline (LR)':<20} {rmse:<10.4f} {accuracy:<10.4f} {f1:<10.4f}\")\n",
    "print(f\"{'PyTorch':<20} {rmse_torch:<10.4f} {accuracy_torch:<10.4f} {f1_torch:<10.4f}\")\n",
    "print(f\"{'Self-Trained':<20} {rmse_combined:<10.4f} {accuracy_combined:<10.4f} {f1_combined:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d25d68-b71a-4f39-b9cc-b60b41d5a419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n",
      "\n",
      "=== Balancing Dataset ===\n",
      "Jumlah sampel terkecil per pilar: 182\n",
      "Data sebelum balancing: 5000 sampel\n",
      "Data setelah balancing: 1456 sampel\n",
      "Distribusi label setelah balancing:\n",
      "label\n",
      "3    182\n",
      "6    182\n",
      "4    182\n",
      "7    182\n",
      "5    182\n",
      "1    182\n",
      "2    182\n",
      "0    182\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data berlabel: 1164 sampel\n",
      "Data tanpa label: 292 sampel\n",
      "\n",
      "Class weights: {0: 0.9965753424657534, 1: 0.9965753424657534, 2: 1.0034482758620689, 3: 1.0034482758620689, 4: 1.0034482758620689, 5: 0.9965753424657534, 6: 1.0034482758620689, 7: 0.9965753424657534}\n",
      "\n",
      "=== Loading IndoBERT ===\n",
      "\n",
      "=== Training IndoBERT Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 2.0477\n",
      "Epoch 1, Batch 10, Loss: 2.1392\n",
      "Epoch 1, Batch 20, Loss: 2.0174\n",
      "Epoch 1, Batch 30, Loss: 1.8460\n",
      "Epoch 1, Batch 40, Loss: 1.7604\n",
      "Epoch 1, Batch 50, Loss: 1.8105\n",
      "Epoch 1, Average Loss: 1.9360\n",
      "Epoch 2, Batch 0, Loss: 1.5338\n",
      "Epoch 2, Batch 10, Loss: 1.6116\n",
      "Epoch 2, Batch 20, Loss: 1.4490\n",
      "Epoch 2, Batch 30, Loss: 1.5346\n",
      "Epoch 2, Batch 40, Loss: 1.3165\n",
      "Epoch 2, Batch 50, Loss: 0.8454\n",
      "Epoch 2, Average Loss: 1.4191\n",
      "Epoch 3, Batch 0, Loss: 1.3090\n",
      "Epoch 3, Batch 10, Loss: 0.9628\n",
      "Epoch 3, Batch 20, Loss: 1.1697\n",
      "Epoch 3, Batch 30, Loss: 0.9242\n",
      "Epoch 3, Batch 40, Loss: 1.2696\n",
      "Epoch 3, Batch 50, Loss: 1.2295\n",
      "Epoch 3, Average Loss: 1.1425\n",
      "\n",
      "=== Evaluating IndoBERT ===\n",
      "IndoBERT Results:\n",
      "Accuracy: 0.5107\n",
      "F1-Score: 0.5014\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     harmoni       0.61      0.48      0.54        29\n",
      "  hilirisasi       0.44      0.52      0.48        29\n",
      "    ideologi       0.54      0.69      0.61        29\n",
      "   pekerjaan       0.46      0.21      0.29        29\n",
      "  pemerataan       0.41      0.41      0.41        29\n",
      "  pertahanan       0.64      0.70      0.67        30\n",
      "   reformasi       0.44      0.55      0.49        29\n",
      "         sdm       0.54      0.52      0.53        29\n",
      "\n",
      "    accuracy                           0.51       233\n",
      "   macro avg       0.51      0.51      0.50       233\n",
      "weighted avg       0.51      0.51      0.50       233\n",
      "\n",
      "\n",
      "=== Self-Training dengan IndoBERT ===\n",
      "Data unlabeled: 292\n",
      "Data dengan confidence tinggi: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 2.1331\n",
      "Epoch 1, Batch 10, Loss: 2.0613\n",
      "Epoch 1, Batch 20, Loss: 1.9908\n",
      "Epoch 1, Batch 30, Loss: 1.9802\n",
      "Epoch 1, Batch 40, Loss: 1.8666\n",
      "Epoch 1, Batch 50, Loss: 1.7567\n",
      "Epoch 1, Batch 60, Loss: 1.5886\n",
      "Epoch 1, Batch 70, Loss: 1.5384\n",
      "Epoch 1, Average Loss: 1.8772\n",
      "Epoch 2, Batch 0, Loss: 1.5390\n",
      "Epoch 2, Batch 10, Loss: 1.4970\n",
      "Epoch 2, Batch 20, Loss: 1.5992\n",
      "Epoch 2, Batch 30, Loss: 1.2285\n",
      "Epoch 2, Batch 40, Loss: 1.3774\n",
      "Epoch 2, Batch 50, Loss: 1.3646\n",
      "Epoch 2, Batch 60, Loss: 1.5065\n",
      "Epoch 2, Batch 70, Loss: 1.4177\n",
      "Epoch 2, Average Loss: 1.3500\n",
      "\n",
      "IndoBERT After Self-Training:\n",
      "Accuracy: 0.6094\n",
      "F1-Score: 0.6066\n",
      "\n",
      "=== RINGKASAN HASIL ===\n",
      "Model                     Accuracy   F1-Score  \n",
      "---------------------------------------------\n",
      "IndoBERT                  0.5107     0.5014    \n",
      "IndoBERT + Self-Training  0.6094     0.6066    \n",
      "\n",
      "Dataset balance information:\n",
      "Total balanced samples: 1456\n",
      "Samples per class: 182\n",
      "Number of classes: 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Cek ketersediaan PyTorch dan GPU\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# 2. Load dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Pastikan label di-encode menjadi numerik\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# 3. Balance dataset - buat semua pilar memiliki jumlah sampel yang sama\n",
    "print(\"\\n=== Balancing Dataset ===\")\n",
    "min_samples = df['label'].value_counts().min()\n",
    "print(f\"Jumlah sampel terkecil per pilar: {min_samples}\")\n",
    "\n",
    "balanced_dfs = []\n",
    "for label in df['label'].unique():\n",
    "    label_df = df[df['label'] == label]\n",
    "    if len(label_df) > min_samples:\n",
    "        # Downsample jika lebih banyak\n",
    "        label_df = resample(label_df, \n",
    "                          n_samples=min_samples, \n",
    "                          random_state=42, \n",
    "                          replace=False)\n",
    "    balanced_dfs.append(label_df)\n",
    "\n",
    "df_balanced = pd.concat(balanced_dfs)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Data sebelum balancing: {len(df)} sampel\")\n",
    "print(f\"Data setelah balancing: {len(df_balanced)} sampel\")\n",
    "print(\"Distribusi label setelah balancing:\")\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "# 4. Split data balanced\n",
    "df_labeled, df_unlabeled = train_test_split(\n",
    "    df_balanced, test_size=0.2, stratify=df_balanced['label'], random_state=42\n",
    ")\n",
    "df_unlabeled = df_unlabeled.copy()\n",
    "df_unlabeled['label'] = np.nan\n",
    "\n",
    "print(f\"\\nData berlabel: {len(df_labeled)} sampel\")\n",
    "print(f\"Data tanpa label: {len(df_unlabeled)} sampel\")\n",
    "\n",
    "# 5. Hitung class weights\n",
    "class_counts = df_labeled['label'].value_counts().sort_index()\n",
    "total_samples = len(df_labeled)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"\\nClass weights: {dict(zip(class_counts.index, class_weights))}\")\n",
    "\n",
    "# 6. Persiapan dataset untuk IndoBERT\n",
    "class IndoBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 7. Load IndoBERT tokenizer dan model\n",
    "print(\"\\n=== Loading IndoBERT ===\")\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Split data untuk IndoBERT\n",
    "X_train_text = df_labeled['text']\n",
    "y_train_text = df_labeled['label']\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    X_train_text, y_train_text, test_size=0.2, stratify=y_train_text, random_state=42\n",
    ")\n",
    "\n",
    "# Buat dataset\n",
    "train_dataset = IndoBERTDataset(X_train_bert, y_train_bert, tokenizer)\n",
    "test_dataset = IndoBERTDataset(X_test_bert, y_test_bert, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 8. Training IndoBERT dengan manual training loop\n",
    "print(\"\\n=== Training IndoBERT Model ===\")\n",
    "\n",
    "num_classes = len(df_labeled['label'].unique())\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Optimizer dan scheduler\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training function\n",
    "def train_bert_model(model, train_loader, optimizer, scheduler, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_bert_model(bert_model, train_loader, optimizer, scheduler, num_epochs=3)\n",
    "\n",
    "# 9. Evaluasi IndoBERT\n",
    "print(\"\\n=== Evaluating IndoBERT ===\")\n",
    "bert_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = bert_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy_bert = accuracy_score(all_labels, all_predictions)\n",
    "f1_bert = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"IndoBERT Results:\")\n",
    "print(f\"Accuracy: {accuracy_bert:.4f}\")\n",
    "print(f\"F1-Score: {f1_bert:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=label_encoder.classes_))\n",
    "\n",
    "# 10. Self-training dengan IndoBERT untuk data unlabeled\n",
    "print(\"\\n=== Self-Training dengan IndoBERT ===\")\n",
    "\n",
    "# Function untuk predict probabilities\n",
    "def predict_proba(texts, model, tokenizer, batch_size=16):\n",
    "    \"\"\"Predict probabilities for a list of texts\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    # Create dataset\n",
    "    class UnlabeledDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_length=128):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten()\n",
    "            }\n",
    "    \n",
    "    dataset = UnlabeledDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "# Prediksi data unlabeled\n",
    "unlabeled_texts = df_unlabeled['text'].tolist()\n",
    "if unlabeled_texts:\n",
    "    pseudo_probs = predict_proba(unlabeled_texts, bert_model, tokenizer)\n",
    "    pseudo_labels = np.argmax(pseudo_probs, axis=1)\n",
    "    \n",
    "    # Filter pseudo-labels dengan confidence tinggi\n",
    "    confidence_threshold = 0.8\n",
    "    high_confidence_mask = pseudo_probs.max(axis=1) >= confidence_threshold\n",
    "    \n",
    "    print(f\"Data unlabeled: {len(unlabeled_texts)}\")\n",
    "    print(f\"Data dengan confidence tinggi: {high_confidence_mask.sum()}\")\n",
    "    \n",
    "    # Gabungkan dengan data labeled jika ada high confidence samples\n",
    "    if high_confidence_mask.sum() > 0:\n",
    "        high_conf_texts = [unlabeled_texts[i] for i in range(len(unlabeled_texts)) if high_confidence_mask[i]]\n",
    "        high_conf_labels = [pseudo_labels[i] for i in range(len(pseudo_labels)) if high_confidence_mask[i]]\n",
    "        \n",
    "        # Buat dataset gabungan\n",
    "        combined_texts = X_train_text.tolist() + high_conf_texts\n",
    "        combined_labels = y_train_text.tolist() + high_conf_labels\n",
    "        \n",
    "        combined_train_dataset = IndoBERTDataset(\n",
    "            pd.Series(combined_texts), \n",
    "            pd.Series(combined_labels), \n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        combined_train_loader = DataLoader(combined_train_dataset, batch_size=16, shuffle=True)\n",
    "        \n",
    "        # Train ulang dengan data gabungan\n",
    "        bert_model_combined = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_classes\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer_combined = AdamW(bert_model_combined.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        total_steps_combined = len(combined_train_loader) * 2  # 2 epochs\n",
    "        scheduler_combined = get_linear_schedule_with_warmup(\n",
    "            optimizer_combined, \n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps_combined\n",
    "        )\n",
    "        \n",
    "        # Train combined model\n",
    "        train_bert_model(bert_model_combined, combined_train_loader, optimizer_combined, scheduler_combined, num_epochs=2)\n",
    "        \n",
    "        # Evaluasi model combined\n",
    "        bert_model_combined.eval()\n",
    "        all_predictions_combined = []\n",
    "        all_labels_combined = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = bert_model_combined(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions_combined.extend(predictions.cpu().numpy())\n",
    "                all_labels_combined.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy_combined = accuracy_score(all_labels_combined, all_predictions_combined)\n",
    "        f1_combined = f1_score(all_labels_combined, all_predictions_combined, average='weighted')\n",
    "        \n",
    "        print(f\"\\nIndoBERT After Self-Training:\")\n",
    "        print(f\"Accuracy: {accuracy_combined:.4f}\")\n",
    "        print(f\"F1-Score: {f1_combined:.4f}\")\n",
    "\n",
    "# 11. Ringkasan hasil\n",
    "print(\"\\n=== RINGKASAN HASIL ===\")\n",
    "print(f\"{'Model':<25} {'Accuracy':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'IndoBERT':<25} {accuracy_bert:<10.4f} {f1_bert:<10.4f}\")\n",
    "\n",
    "if 'accuracy_combined' in locals():\n",
    "    print(f\"{'IndoBERT + Self-Training':<25} {accuracy_combined:<10.4f} {f1_combined:<10.4f}\")\n",
    "\n",
    "print(f\"\\nDataset balance information:\")\n",
    "print(f\"Total balanced samples: {len(df_balanced)}\")\n",
    "print(f\"Samples per class: {min_samples}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d6e77-d1fc-4ec2-905d-edf738ca848a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset balanced: label\n",
      "1    2000\n",
      "3    2000\n",
      "7    2000\n",
      "2    2000\n",
      "4    2000\n",
      "0    2000\n",
      "5    2000\n",
      "6    2000\n",
      "Name: count, dtype: int64\n",
      "Starting enhanced training...\n",
      "\n",
      "=== Fold 1/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Clean text data\n",
    "    df['text'] = df['text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    df['text'] = df['text'].str.replace(r'\\d+', '', regex=True)\n",
    "    df['text'] = df['text'].str.strip()\n",
    "    df = df[df['text'].str.len() > 10]  # Remove very short texts\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])\n",
    "    \n",
    "    return df, label_encoder\n",
    "\n",
    "df, label_encoder = load_and_preprocess_data('train.csv')\n",
    "\n",
    "# 3. Advanced balancing with augmentation\n",
    "def balance_dataset(df, target_samples_per_class=2000):\n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for label in df['label'].unique():\n",
    "        label_df = df[df['label'] == label].copy()\n",
    "        \n",
    "        if len(label_df) < target_samples_per_class:\n",
    "            # Upsample with replacement\n",
    "            label_df = resample(label_df,\n",
    "                              n_samples=target_samples_per_class,\n",
    "                              random_state=42,\n",
    "                              replace=True,\n",
    "                              stratify=label_df['label'] if len(label_df) > 1 else None)\n",
    "        else:\n",
    "            # Downsample\n",
    "            label_df = resample(label_df,\n",
    "                              n_samples=target_samples_per_class,\n",
    "                              random_state=42,\n",
    "                              replace=False)\n",
    "        \n",
    "        balanced_dfs.append(label_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = balance_dataset(df, target_samples_per_class=2000)\n",
    "print(\"Dataset balanced:\", balanced_df['label'].value_counts())\n",
    "\n",
    "# 4. Enhanced IndoBERT Dataset with data augmentation\n",
    "class EnhancedIndoBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256, augment=False):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        # Simple text augmentation\n",
    "        if self.augment and np.random.random() > 0.7:\n",
    "            words = text.split()\n",
    "            if len(words) > 5:\n",
    "                # Random swap\n",
    "                if np.random.random() > 0.5 and len(words) > 2:\n",
    "                    i, j = np.random.choice(len(words), 2, replace=False)\n",
    "                    words[i], words[j] = words[j], words[i]\n",
    "                    text = ' '.join(words)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 5. Improved training function with early stopping and validation\n",
    "def train_bert_model_with_validation(model, train_loader, val_loader, optimizer, scheduler, num_epochs=10, patience=3):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_val_loss += outputs.loss.item()\n",
    "                \n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_indobert_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# 6. Cross-validation training\n",
    "def cross_validate_bert(df, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    texts = df['text']\n",
    "    labels = df['label']\n",
    "    \n",
    "    all_accuracies = []\n",
    "    all_f1_scores = []\n",
    "    \n",
    "    model_name = \"indobenchmark/indobert-base-p1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        print(f\"\\n=== Fold {fold + 1}/{n_splits} ===\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = texts.iloc[train_idx], texts.iloc[val_idx]\n",
    "        y_train, y_val = labels.iloc[train_idx], labels.iloc[val_idx]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = EnhancedIndoBERTDataset(X_train, y_train, tokenizer, augment=True)\n",
    "        val_dataset = EnhancedIndoBERTDataset(X_val, y_val, tokenizer, augment=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        num_classes = len(df['label'].unique())\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_classes\n",
    "        ).to(device)\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        \n",
    "        # Scheduler\n",
    "        total_steps = len(train_loader) * 10\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=int(0.1 * total_steps),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        train_losses, val_losses, val_accuracies = train_bert_model_with_validation(\n",
    "            model, train_loader, val_loader, optimizer, scheduler, num_epochs=10\n",
    "        )\n",
    "        \n",
    "        # Load best model for evaluation\n",
    "        model.load_state_dict(torch.load('best_indobert_model.pth'))\n",
    "        model.eval()\n",
    "        \n",
    "        # Final evaluation\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_true, all_preds)\n",
    "        f1 = f1_score(all_true, all_preds, average='weighted')\n",
    "        \n",
    "        all_accuracies.append(accuracy)\n",
    "        all_f1_scores.append(f1)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(classification_report(all_true, all_preds, target_names=label_encoder.classes_))\n",
    "    \n",
    "    print(f\"\\n=== Cross-Validation Results ===\")\n",
    "    print(f\"Average Accuracy: {np.mean(all_accuracies):.4f} (+/- {np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Average F1-Score: {np.mean(all_f1_scores):.4f} (+/- {np.std(all_f1_scores):.4f})\")\n",
    "    \n",
    "    return np.mean(all_accuracies), np.mean(all_f1_scores)\n",
    "\n",
    "# 7. Train final model on all data\n",
    "def train_final_model(df):\n",
    "    model_name = \"indobenchmark/indobert-base-p1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Split final train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['text'], df['label'], test_size=0.1, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = EnhancedIndoBERTDataset(X_train, y_train, tokenizer, augment=True)\n",
    "    val_dataset = EnhancedIndoBERTDataset(X_val, y_val, tokenizer, augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    num_classes = len(df['label'].unique())\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    total_steps = len(train_loader) * 15\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    print(\"=== Training Final Model ===\")\n",
    "    train_losses, val_losses, val_accuracies = train_bert_model_with_validation(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, num_epochs=15, patience=5\n",
    "    )\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_indobert_model.pth'))\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 8. Main execution\n",
    "print(\"Starting enhanced training...\")\n",
    "\n",
    "# Cross-validation first\n",
    "avg_accuracy, avg_f1 = cross_validate_bert(balanced_df, n_splits=3)\n",
    "\n",
    "if avg_accuracy > 0.8:  # If CV results are good, train final model\n",
    "    print(\"\\nCV results are good, training final model...\")\n",
    "    final_model, tokenizer = train_final_model(balanced_df)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(final_model.state_dict(), 'final_indobert_model.pth')\n",
    "    tokenizer.save_pretrained('./tokenizer')\n",
    "    \n",
    "    print(\"Final model trained and saved successfully!\")\n",
    "else:\n",
    "    print(\"\\nCV results are poor. Consider:\")\n",
    "    print(\"1. Adding more data\")\n",
    "    print(\"2. Trying different model architecture\")\n",
    "    print(\"3. Checking data quality\")\n",
    "    print(\"4. Adjusting hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3681cb3-577a-4ea9-9d1e-98cbb6dc1352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
